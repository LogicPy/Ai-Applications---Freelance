# data_processing/preprocess.py

import nltk
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression  # Example classifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import os

# Initialize NLTK resources
nltk.download('punkt')

# Paths
VECTORIZER_PATH = os.path.join('models', 'vectorizer.pkl')
MODEL_PATH = os.path.join('models', 'model_name.pkl')  # Using .pkl for scikit-learn
TRAINING_DATA_PATH = os.path.join('training.txt')

# data_processing/preprocess.py

import joblib
import os
from sklearn.feature_extraction.text import TfidfVectorizer


def load_vectorizer():
    """
    Loads the TF-IDF vectorizer from disk or fits a new one if not present.
    
    Returns:
        vectorizer: The loaded or newly fitted TF-IDF vectorizer.
    """
    if os.path.exists(VECTORIZER_PATH):
        vectorizer = joblib.load(VECTORIZER_PATH)
        print("Vectorizer loaded successfully.")
    else:
        # If vectorizer doesn't exist, fit a new one
        from sklearn.feature_extraction.text import TfidfVectorizer
        TRAINING_DATA_PATH = os.path.join('training.txt')
        if not os.path.exists(TRAINING_DATA_PATH):
            raise FileNotFoundError(f"Training data file not found at {TRAINING_DATA_PATH}")
        with open(TRAINING_DATA_PATH, 'r', encoding='utf-8') as file:
            data = file.read().split('\n\n')  # Assuming double newline separates entries
        texts = []
        for entry in data:
            if not entry.strip():
                continue
            lines = entry.strip().split('\n')
            text = ' '.join(lines[1:]).strip()  # Remaining lines are description
            texts.append(text)
        vectorizer = TfidfVectorizer(stop_words='english')
        vectorizer.fit(texts)
        joblib.dump(vectorizer, VECTORIZER_PATH)
        print("Vectorizer fitted and saved successfully.")
    return vectorizer



def load_or_train_model(vectorizer):
    """
    Loads the trained model from disk or trains a new one if not present.
    """
    if os.path.exists(MODEL_PATH):
        model = joblib.load(MODEL_PATH)
        print("Model loaded successfully.")
    else:
        if not os.path.exists(TRAINING_DATA_PATH):
            raise FileNotFoundError(f"Training data file not found at {TRAINING_DATA_PATH}")
        with open(TRAINING_DATA_PATH, 'r', encoding='utf-8') as file:
            data = file.read().split('\n\n')  # Assuming double newline separates entries
        texts = []
        labels = []
        for entry in data:
            if not entry.strip():
                continue
            lines = entry.strip().split('\n')
            label = lines[0].strip()  # Assuming first line is label
            text = ' '.join(lines[1:]).strip()  # Remaining lines are description
            texts.append(text)
            labels.append(label)
        X = vectorizer.transform(texts)
        y = labels
        # Example using Logistic Regression; replace with your chosen model
        model = LogisticRegression(max_iter=1000)
        model.fit(X, y)
        joblib.dump(model, MODEL_PATH)
        print("Model trained and saved successfully.")
    return model

# Load or train the vectorizer and model
vectorizer = load_vectorizer()
model = load_or_train_model(vectorizer)

def preprocess_input_data(text_data):
    """
    Preprocesses the input text data by tokenizing and vectorizing.

    Args:
        text_data (str): The input text.

    Returns:
        scipy.sparse.csr.csr_matrix: The vectorized representation of the input text.
    """
    tokens = word_tokenize(text_data)  # Tokenize the text
    # Vectorize the text using the loaded TF-IDF vectorizer
    vectorized_data = vectorizer.transform([text_data])
    return vectorized_data
